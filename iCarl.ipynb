{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random \n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataset import Cifar100\n",
    "from resnet_cifar import resnet32\n",
    "from utils import parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "LR = parameters['LR']\n",
    "WEIGHT_DECAY = parameters['WEIGHT_DECAY']\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = parameters['NUM_EPOCHS']\n",
    "DEVICE = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE ='cuda' \n",
    "MOMENTUM=parameters['MOMENTUM']\n",
    "MILESTONES=parameters['MILESTONES']\n",
    "CLASSES_BATCH=parameters['CLASSES_BATCH']\n",
    "NUM_CLASSES=parameters['NUM_CLASSES']\n",
    "STEPDOWN_FACTOR=parameters['STEPDOWN_FACTOR']\n",
    "GAMMA=parameters['GAMMA']\n",
    "CRITERION=nn.BCEWithLogitsLoss()\n",
    "MEMORY=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing iCarl_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iCarl_model.py\n",
    "class iCaRL():\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        self.memory = MEMORY\n",
    "        self.params = params\n",
    "        self.device = DEVICE\n",
    "        \n",
    "        \n",
    "    def train(self, net, old_net, train_dataloader, optimizer, n_epochs, n_classes):\n",
    "\n",
    "      criterion = nn.BCEWithLogitsLoss()\n",
    "      parameters_to_optimize = net.parameters()\n",
    "\n",
    "      train_losses = []\n",
    "\n",
    "      net.to(self.device)\n",
    "\n",
    "      for epoch in range(n_epochs):\n",
    "\n",
    "        if epoch in MILESTONES:\n",
    "          for pg in optimizer.param_groups:\n",
    "            pg['lr'] = pg['lr']/self.params['STEPDOWN_FACTOR']\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for indexes, inputs, labels in train_dataloader:\n",
    "          inputs = inputs.to(self.device)\n",
    "          labels = labels.to(self.device)\n",
    "          \n",
    "          labels_hot=torch.eye(n_classes)[labels]\n",
    "          labels_hot = labels_hot.to(self.device)\n",
    "\n",
    "          net.train(True)\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          outputs = net(inputs)\n",
    "\n",
    "          if n_classes == 10:\n",
    "            loss = criterion(outputs[:, n_classes - 10:], labels_hot[:, n_classes - 10:])\n",
    "          else:\n",
    "            old_outputs = self.get_old_outputs(inputs, old_net)\n",
    "            targets = torch.cat((old_outputs, labels_hot[:, n_classes - 10:]), 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # statistics\n",
    "          running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == (n_epochs-1):\n",
    "          print('Epoch {} Loss:{:.4f}'.format(epoch, epoch_loss))\n",
    "          for pg in optimizer.param_groups:\n",
    "            print('Learning rate:{}'.format(pg['lr']))\n",
    "          print('-'*30)\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "      return net, train_losses\n",
    "    def NME(self, data, exemplars, net, n_classes):\n",
    "      print('-'*30)\n",
    "      print(f'**** Classification: NME ****')\n",
    "      print('-'*30)\n",
    "      \n",
    "      means = dict.fromkeys(np.arange(n_classes))\n",
    "      net.eval()\n",
    "\n",
    "      # compute exemplars prototypes\n",
    "      print(f'**** computing means of exemplars... ****')\n",
    "      print('-'*30)\n",
    "      for key in exemplars:\n",
    "        \n",
    "        loader = DataLoader(exemplars[key], batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "        mean = torch.zeros((1,64), device=self.device)\n",
    "        \n",
    "        for _, images, _ in loader:\n",
    "          with torch.no_grad():\n",
    "\n",
    "            images = images.to(self.device)\n",
    "            outputs = net(images, features=True)\n",
    "            \n",
    "            for output in outputs:\n",
    "              mean += output\n",
    "        \n",
    "        mean = mean / len(exemplars[key])\n",
    "        means[key] = mean / mean.norm()\n",
    "\n",
    "      # applying nme classification\n",
    "      print(f'**** predicting... ****')\n",
    "      print('-'*30)\n",
    "\n",
    "      loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "      running_correct = 0.0\n",
    "      for _, images, labels in loader:\n",
    "\n",
    "        images = images.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          \n",
    "          outputs = net(images, features=True)\n",
    "          preds = []\n",
    "          \n",
    "          for output in outputs:\n",
    "            \n",
    "            pred = None\n",
    "            min_dist = float('inf')\n",
    "            \n",
    "            for key in means:\n",
    "              dist = torch.dist(means[key], output)\n",
    "              if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                pred = key\n",
    "            \n",
    "            preds.append(pred)\n",
    "          \n",
    "          for label, pred in zip(labels,preds):\n",
    "            if label == pred:\n",
    "              running_correct += 1\n",
    "      \n",
    "      accuracy = running_correct / len(data)\n",
    "      print('Accuracy:{:.4f}'.format(accuracy))\n",
    "\n",
    "      return accuracy\n",
    "    \n",
    "    def update_net(self, net, n_classes):\n",
    "      in_features = net.fc.in_features\n",
    "      out_features = net.fc.out_features\n",
    "      weight = net.fc.weight.data\n",
    "      bias = net.fc.bias.data\n",
    "\n",
    "      net.fc = nn.Linear(in_features, n_classes)\n",
    "      net.fc.weight.data[:out_features] = weight\n",
    "      net.fc.bias.data[:out_features] = bias\n",
    "\n",
    "      return net\n",
    "    \n",
    "    \n",
    "    def random_exemplar(self, data, n_classes):\n",
    "      print('-'*30)\n",
    "      print(f'**** Construct new exemplars: Random mode ****')\n",
    "      print('-'*30)\n",
    "\n",
    "      m = int(self.memory / n_classes)\n",
    "\n",
    "      sample_per_class = dict.fromkeys(np.arange(n_classes - 10, n_classes))\n",
    "      exemplars = dict.fromkeys(np.arange(n_classes - 10, n_classes))\n",
    "\n",
    "      for label in sample_per_class:\n",
    "          sample_per_class[label] = []\n",
    "          exemplars[label] = []\n",
    "\n",
    "      for item in data:\n",
    "          for label in sample_per_class:\n",
    "            if item[2] == label:\n",
    "              sample_per_class[label].append(item)\n",
    "\n",
    "      for label in range(n_classes - 10, n_classes):\n",
    "        \n",
    "        indexes = random.sample(range(len(sample_per_class[label])), m)\n",
    "        \n",
    "        for i in indexes:\n",
    "          exemplars[label].append(sample_per_class[label][i])\n",
    "\n",
    "      return exemplars\n",
    "    \n",
    "    def herding_exemplar(self, data, n_classes, net):\n",
    "        print('-'*30)\n",
    "        print(f'**** Construct new exemplars: Herding mode ****')\n",
    "        print('-'*30)\n",
    "\n",
    "        m = int(self.memory / n_classes)\n",
    "\n",
    "        means = dict.fromkeys(np.arange(n_classes - 10, n_classes))\n",
    "        sample_per_class = dict.fromkeys(np.arange(n_classes - 10, n_classes))\n",
    "        exemplars = dict.fromkeys(np.arange(n_classes - 10, n_classes))\n",
    "\n",
    "        for label in sample_per_class:\n",
    "          sample_per_class[label] = []\n",
    "          exemplars[label] = []\n",
    "          means[label] = []\n",
    "        \n",
    "        for item in data:\n",
    "          for label in sample_per_class:\n",
    "            if item[2] == label:\n",
    "              sample_per_class[label].append(item)\n",
    "        \n",
    "        # generate new exemplars\n",
    "        net.eval()\n",
    "        for label in sample_per_class:\n",
    "          # initialize mean tensor, is a single value with a number of components equal to the number of outputs of the last conv layer of the resnet32\n",
    "          mean = torch.zeros((1,64), device=self.device)\n",
    "          data_features = []\n",
    "          \n",
    "          # compute means of data features for each class \n",
    "          with torch.no_grad():\n",
    "            loader = DataLoader(sample_per_class[label], batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "            for _, images, _ in loader:\n",
    "                \n",
    "                images = images.to(self.device)\n",
    "                outputs = net(images,features=True)\n",
    "                \n",
    "                for output in outputs:\n",
    "                    output = output.to(self.device)\n",
    "                    \n",
    "                    # save the data features to use them also in the examplar selection\n",
    "                    data_features.append(output)\n",
    "                    mean += output\n",
    "            \n",
    "            mean = mean / len(sample_per_class[label])\n",
    "            # normalize the mean\n",
    "            means[label] = mean / mean.norm()\n",
    "          \n",
    "          # find the m sample features which mean is the closest to the one of the entire class\n",
    "          \n",
    "          exemplars_features = []\n",
    "          min_index = 0\n",
    "          for i in range(m):\n",
    "            \n",
    "            min_distance = float('inf')\n",
    "            exemplar_sum = sum(exemplars_features)\n",
    "            \n",
    "            for idx, data_feature in enumerate(data_features):\n",
    "              \n",
    "              tmp_mean = (exemplar_sum + data_feature) / (len(exemplars_features) + 1)\n",
    "              # normalize the mean\n",
    "              tmp_mean = tmp_mean / tmp_mean.norm()\n",
    "              \n",
    "              if torch.dist(mean, tmp_mean) < min_distance:\n",
    "                min_distance = torch.dist(mean, tmp_mean)\n",
    "                min_index = idx\n",
    "               \n",
    "            exemplars[label].append(sample_per_class[label][min_index])\n",
    "            exemplars_features.append(data_features[min_index])\n",
    "            sample_per_class[label].pop(min_index)\n",
    "            data_features.pop(min_index)\n",
    "\n",
    "        return exemplars\n",
    "    def update_representation(self, new_data, exemplars, net, n_classes):\n",
    "        '''\n",
    "        X= training iamges of classes s....t\n",
    "        P=(P1,....,P_s-1) #exemplars sets\n",
    "        theta #current model parameters\n",
    "        '''\n",
    "        print('-'*30)\n",
    "        print(f'**** Update Representation... ****')\n",
    "        print('-'*30)\n",
    "        \n",
    "        # concatenate new data with set of exemplars\n",
    "        if len(exemplars) != 0:\n",
    "          data = new_data + exemplars\n",
    "        else:\n",
    "          data = new_data\n",
    "        \n",
    "        old_net = deepcopy(net) #salva network \n",
    "        \n",
    "        loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "        if n_classes != 10:\n",
    "          # update net last layer\n",
    "          net = self.update_net(net, n_classes)\n",
    "          \n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=self.params['LR'], momentum=self.params['MOMENTUM'], weight_decay=self.params['WEIGHT_DECAY'])\n",
    "        \n",
    "        net, train_losses = self.train(net, old_net, loader, optimizer, self.params['NUM_EPOCHS'], n_classes)\n",
    "\n",
    "        return net, train_losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplars = {}\n",
    "new_exemplars = []\n",
    "exemplars_as_list = []\n",
    "accuracy_new = []\n",
    "accuracy_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for training phase\n",
    "transform_train = transforms.Compose([\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                  ])\n",
    "transform_test = transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet32()\n",
    "from iCarl_model import iCaRL\n",
    "icarl=iCaRL()\n",
    "for i in range(int(NUM_CLASSES/CLASSES_BATCH)):\n",
    "    print('-'*30)\n",
    "    print(f'**** ITERATION {i+1} ****')\n",
    "    print('-'*30)\n",
    "\n",
    "    n_classes = (i+1)*10 #10-20-30....\n",
    "\n",
    "    train_dataset = Cifar100(classes=range(i*10, (i + 1)*10), train=True, transform=transform_train)\n",
    "    test_dataset = Cifar100(classes=range(i*10, (i + 1)*10), train=False, transform=transform_test)\n",
    "\n",
    "    # update representation\n",
    "    net, train_losses = self.update_representation(train_dataset, exemplars_as_list, net, n_classes)\n",
    "    print(train_losses)\n",
    "    break\n",
    "    #rappresentazione plot loss\n",
    "    parameters['name']='icarl_loss'\n",
    "    \n",
    "\n",
    "    # update exemplar sets\n",
    "    exemplars = self.reduce_exemplar(exemplars, n_classes)\n",
    "\n",
    "    if herding:\n",
    "      new_exemplars = self.herding_exemplar(train_dataset, n_classes, net)\n",
    "    else:\n",
    "      new_exemplars = self.random_exemplar(train_dataset, n_classes)\n",
    "\n",
    "    exemplars.update(new_exemplars)\n",
    "\n",
    "    exemplars_as_list = [item for class_exemplars in exemplars.values() for item in class_exemplars]\n",
    "\n",
    "    # compute accuracy on the new class batch\n",
    "    accuracy_new.append(self.NME(test_dataset, exemplars, net, n_classes))\n",
    "\n",
    "    # compute accuracy on all the classes seen so far\n",
    "    test_dataset_sofar = Cifar100(classes=range(0, (i + 1)*10), train=False, transform=transform_test)\n",
    "    accuracy_all.append(self.NME(test_dataset_sofar, exemplars, net, n_classes))\n",
    "\n",
    "   # return accuracy_new, accuracy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(NUM_CLASSES/CLASSES_BATCH)):\n",
    "    print((i+1)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
