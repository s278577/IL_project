{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random \n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataset import Cifar100\n",
    "from resnet_cifar import resnet32\n",
    "from utils import parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "LR = parameters['LR']\n",
    "WEIGHT_DECAY = parameters['WEIGHT_DECAY']\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = parameters['NUM_EPOCHS']\n",
    "DEVICE = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE ='cuda' \n",
    "MOMENTUM=parameters['MOMENTUM']\n",
    "MILESTONES=parameters['MILESTONES']\n",
    "CLASSES_BATCH=parameters['CLASSES_BATCH']\n",
    "NUM_CLASSES=parameters['NUM_CLASSES']\n",
    "STEPDOWN_FACTOR=parameters['STEPDOWN_FACTOR']\n",
    "GAMMA=parameters['GAMMA']\n",
    "CRITERION=nn.BCEWithLogitsLoss()\n",
    "MEMORY=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iCaRL():\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        self.memory = MEMORY\n",
    "        self.params = params\n",
    "        self.device = DEVICE\n",
    "        \n",
    "        \n",
    "    def train(self, net, old_net, train_dataloader, optimizer, n_epochs, n_classes):\n",
    "\n",
    "      criterion = nn.BCEWithLogitsLoss()\n",
    "      parameters_to_optimize = net.parameters()\n",
    "\n",
    "      train_losses = []\n",
    "\n",
    "      net.to(self.device)\n",
    "\n",
    "      for epoch in range(n_epochs):\n",
    "\n",
    "        if epoch in MILESTONES:\n",
    "          for pg in optimizer.param_groups:\n",
    "            pg['lr'] = pg['lr']/self.params['STEPDOWN_FACTOR']\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for indexes, inputs, labels in train_dataloader:\n",
    "          inputs = inputs.to(self.device)\n",
    "          labels = labels.to(self.device)\n",
    "          \n",
    "          labels_hot=torch.eye(n_classes)[labels]\n",
    "          labels_hot = labels_hot.to(self.device)\n",
    "\n",
    "          net.train(True)\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "          # forward\n",
    "          outputs = net(inputs)\n",
    "\n",
    "          if n_classes == 10:\n",
    "            loss = criterion(outputs[:, n_classes - 10:], labels_hot[:, n_classes - 10:])\n",
    "          else:\n",
    "            old_outputs = self.get_old_outputs(inputs, old_net)\n",
    "            targets = torch.cat((old_outputs, labels_hot[:, n_classes - 10:]), 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # statistics\n",
    "          running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == (n_epochs-1):\n",
    "          print('Epoch {} Loss:{:.4f}'.format(epoch, epoch_loss))\n",
    "          for pg in optimizer.param_groups:\n",
    "            print('Learning rate:{}'.format(pg['lr']))\n",
    "          print('-'*30)\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "      return net, train_losses\n",
    "   \n",
    "    #ALGO 3\n",
    "    def update_representation(self, new_data, exemplars, net, n_classes):\n",
    "        '''\n",
    "        X= training iamges of classes s....t\n",
    "        P=(P1,....,P_s-1) #exemplars sets\n",
    "        theta #current model parameters\n",
    "        '''\n",
    "        print('-'*30)\n",
    "        print(f'**** Update Representation... ****')\n",
    "        print('-'*30)\n",
    "        \n",
    "        # concatenate new data with set of exemplars\n",
    "        if len(exemplars) != 0:\n",
    "          data = new_data + exemplars\n",
    "        else:\n",
    "          data = new_data\n",
    "        \n",
    "        old_net = deepcopy(net) #salva network \n",
    "        \n",
    "        loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "        if n_classes != 10:\n",
    "          # update net last layer\n",
    "          net = self.update_net(net, n_classes)\n",
    "          \n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=self.params['LR'], momentum=self.params['MOMENTUM'], weight_decay=self.params['WEIGHT_DECAY'])\n",
    "        \n",
    "        net, train_losses = self.train(net, old_net, loader, optimizer, self.params['NUM_EPOCHS'], n_classes)\n",
    "\n",
    "        return net, train_losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplars = {}\n",
    "new_exemplars = []\n",
    "exemplars_as_list = []\n",
    "accuracy_new = []\n",
    "accuracy_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for training phase\n",
    "transform_train = transforms.Compose([\n",
    "                                    transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                  ])\n",
    "transform_test = transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "**** ITERATION 1 ****\n",
      "------------------------------\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to Dataset/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7350189e9a4f499f85949ba9e82eb4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Dataset/cifar-100-python.tar.gz to Dataset\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4df73a8e811b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# update representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplars_as_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "net = resnet32()\n",
    "icarl=iCarl\n",
    "for i in range(int(NUM_CLASSES/CLASSES_BATCH)):\n",
    "    print('-'*30)\n",
    "    print(f'**** ITERATION {i+1} ****')\n",
    "    print('-'*30)\n",
    "\n",
    "    n_classes = (i+1)*10 #10-20-30....\n",
    "\n",
    "    train_dataset = Cifar100(classes=range(i*10, (i + 1)*10), train=True, transform=transform_train)\n",
    "    test_dataset = Cifar100(classes=range(i*10, (i + 1)*10), train=False, transform=transform_test)\n",
    "\n",
    "    # update representation\n",
    "    net, train_losses = self.update_representation(train_dataset, exemplars_as_list, net, n_classes)\n",
    "    print(train_losses)\n",
    "    break\n",
    "    #rappresentazione plot loss\n",
    "    parameters['name']='icarl_loss'\n",
    "    \n",
    "\n",
    "    # update exemplar sets\n",
    "    exemplars = self.reduce_exemplar(exemplars, n_classes)\n",
    "\n",
    "    if herding:\n",
    "      new_exemplars = self.herding_exemplar(train_dataset, n_classes, net)\n",
    "    else:\n",
    "      new_exemplars = self.random_exemplar(train_dataset, n_classes)\n",
    "\n",
    "    exemplars.update(new_exemplars)\n",
    "\n",
    "    exemplars_as_list = [item for class_exemplars in exemplars.values() for item in class_exemplars]\n",
    "\n",
    "    # compute accuracy on the new class batch\n",
    "    accuracy_new.append(self.NME(test_dataset, exemplars, net, n_classes))\n",
    "\n",
    "    # compute accuracy on all the classes seen so far\n",
    "    test_dataset_sofar = Cifar100(classes=range(0, (i + 1)*10), train=False, transform=transform_test)\n",
    "    accuracy_all.append(self.NME(test_dataset_sofar, exemplars, net, n_classes))\n",
    "\n",
    "   # return accuracy_new, accuracy_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(NUM_CLASSES/CLASSES_BATCH)):\n",
    "    print((i+1)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.4-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
