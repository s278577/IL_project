{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "language": "python",
      "name": "python38264bitb1f90dffcb1c41518b6c4d21c2fb1bef"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "56xP6gUmXnTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLJ06xObXnT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.datasets.utils import check_integrity, download_and_extract_archive\n",
        "\n",
        "\n",
        "class CIFAR10(VisionDataset):\n",
        "   \n",
        "    base_folder = 'cifar-10-batches-py'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    filename = \"cifar-10-python.tar.gz\"\n",
        "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
        "    train_list = [\n",
        "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
        "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
        "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
        "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
        "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
        "    ]\n",
        "    meta = {\n",
        "        'filename': 'batches.meta',\n",
        "        'key': 'label_names',\n",
        "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
        "    }\n",
        "\n",
        "    def __init__(self, root, classes=np.arange(10), train=True, transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "\n",
        "        super(CIFAR10, self).__init__(root, transform=transform,\n",
        "                                      target_transform=target_transform)\n",
        "\n",
        "        self.train = train  # training set or test set\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "\n",
        "        if self.train:\n",
        "            downloaded_list = self.train_list\n",
        "        else:\n",
        "            downloaded_list = self.test_list\n",
        "\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        for file_name, checksum in downloaded_list:\n",
        "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
        "            with open(file_path, 'rb') as f:\n",
        "                entry = pickle.load(f, encoding='latin1')\n",
        "                self.data.append(entry['data'])\n",
        "                if 'labels' in entry:\n",
        "                    self.targets.extend(entry['labels'])\n",
        "                else:\n",
        "                    self.targets.extend(entry['fine_labels'])\n",
        "\n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
        "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "        data = []\n",
        "        targets = []\n",
        "\n",
        "        for i in range(len(self)):\n",
        "            if self.targets[i] in classes:\n",
        "                data.append(self.data[i])\n",
        "                targets.append(self.targets[i])\n",
        "\n",
        "        self.data=np.array(data)\n",
        "        self.targets=targets\n",
        "\n",
        "\n",
        "    def _load_meta(self):\n",
        "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
        "        if not check_integrity(path, self.meta['md5']):\n",
        "            raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "        with open(path, 'rb') as infile:\n",
        "            data = pickle.load(infile, encoding='latin1')\n",
        "            self.classes = data[self.meta['key']]\n",
        "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target, index\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in (self.train_list + self.test_list):\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, self.base_folder, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def download(self):\n",
        "        if self._check_integrity():\n",
        "            print('Files already downloaded and verified')\n",
        "            return\n",
        "        download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return \"Split: {}\".format(\"Train\" if self.train is True else \"Test\")\n",
        "\n",
        "    def append(self, data, targets):\n",
        "        self.data = np.concatenate((self.data, data))\n",
        "        self.targets = np.concatenate((self.targets, targets))\n",
        "\n",
        "    def get_class_imgs(self, target):\n",
        "        images = []\n",
        "        for i, img in enumerate(self.data):\n",
        "            if self.targets[i] == target:\n",
        "                images.append(img)\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "class CIFAR100(CIFAR10):\n",
        "    base_folder = 'cifar-100-python'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
        "    filename = \"cifar-100-python.tar.gz\"\n",
        "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
        "    train_list = [\n",
        "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
        "    ]\n",
        "    meta = {\n",
        "        'filename': 'meta',\n",
        "        'key': 'fine_label_names',\n",
        "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
        "    }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZL7kFIWXnUG",
        "colab_type": "code",
        "colab": {},
        "outputId": "08b3da1a-3cb7-4cea-80cd-878b28b5f9a2"
      },
      "source": [
        "import torch\n",
        "a = torch.randn([2, 3, 4])\n",
        "b = torch.randn([2, 3])\n",
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2462,  0.7337, -0.4384],\n",
              "        [ 0.4247, -0.2700,  0.2603]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOGqPvkpXnUT",
        "colab_type": "code",
        "colab": {},
        "outputId": "419e6c22-6e3b-4634-f330-66e507db65e9"
      },
      "source": [
        "b = b.unsqueeze(0)\n",
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2462,  0.7337, -0.4384],\n",
              "         [ 0.4247, -0.2700,  0.2603]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PBs3hlSXnUd",
        "colab_type": "code",
        "colab": {},
        "outputId": "8af09016-1271-4dc7-e1fa-23e26b58097d"
      },
      "source": [
        "b.squeeze()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2462,  0.7337, -0.4384],\n",
              "        [ 0.4247, -0.2700,  0.2603]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hTnwRo9XnUp",
        "colab_type": "code",
        "colab": {},
        "outputId": "995a0c4d-7aee-432c-f596-79ddf152dc4b"
      },
      "source": [
        "b.mean(0).squeeze()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2462,  0.7337, -0.4384],\n",
              "        [ 0.4247, -0.2700,  0.2603]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWVx9dFpXnUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzlTIB3EXnU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfBzGkpPXnVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from resnet_cifar import resnet32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1CMnnP9XnVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#resnet implementation like prof one!\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APe0G2upXnVR",
        "colab_type": "text"
      },
      "source": [
        "# finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_PkCSEmXnVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "25860c85-997b-4162-a466-8509428ee211"
      },
      "source": [
        "!pip3 install 'livelossplot'\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,Subset\n",
        "from livelossplot import PlotLosses\n",
        "from torch.backends import cudnn \n",
        "#from resnet import resnet32\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models import resnet18\n",
        "import copy"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: livelossplot in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from livelossplot) (3.2.1)\n",
            "Requirement already satisfied: bokeh; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from livelossplot) (1.4.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from livelossplot) (5.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.6\"->livelossplot) (1.18.4)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (7.0.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (20.4)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (4.5.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (1.12.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (3.13)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot) (2.11.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (47.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->livelossplot) (1.0.18)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh; python_version >= \"3.6\"->livelossplot) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->livelossplot) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->livelossplot) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qTo4AzsXnVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyper-parameters\n",
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "ClASSES_BATCH =10\n",
        "LR = 0.01 #default \n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-5\n",
        "NUM_EPOCHS = 70\n",
        "STEP_SIZE= 49 # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA=0.1\n",
        "DRAW=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AzVoT6eXnVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#definition of train and test functions\n",
        "liveloss=PlotLosses()\n",
        "BEST_ACC=0\n",
        "best_net_acc=None\n",
        "logs={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfzjpzb8XnVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#train function + validation\n",
        "def train(net, train_dataloader,val_dataloader):\n",
        "  best_acc=BEST_ACC\n",
        "  cudnn.benchmark #optimizes benchmark\n",
        "  criterion = nn.CrossEntropyLoss() # for classification, Cross Entropy\n",
        "  #criterion = nn.BCELoss()#binary CrossEntropyLoss\n",
        "  parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters \n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "  net.to(DEVICE)\n",
        "\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      logs={}\n",
        "      if(epoch%5 == 0 ):\n",
        "        print('-' * 30)\n",
        "        print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n",
        "        for param_group in optimizer.param_groups:\n",
        "          print('Learning rate:{}'.format(param_group['lr']))\n",
        "      \n",
        "      running_loss = 0.0\n",
        "      val_loss = 0.0\n",
        "      running_corrects_train = 0\n",
        "      running_corrects_val = 0\n",
        "      #train\n",
        "      for inputs, labels, index in train_dataloader:\n",
        "          inputs = inputs.to(DEVICE)\n",
        "          labels = labels.to(DEVICE)\n",
        "\n",
        "          net.train(True)\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          outputs = net(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward() #backward pass: compute gradients\n",
        "          optimizer.step() #update weights based on accumulated gradients\n",
        "\n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects_train += torch.sum(preds == labels.data)\n",
        "\n",
        "\n",
        "      #validation\n",
        "      \n",
        "      for inputs,labels,index in val_dataloader:\n",
        "        inputs,labels=inputs.to(DEVICE),labels.to(DEVICE)\n",
        "        net.train(False)\n",
        "        outputs = net(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss+=loss.item() * inputs.size(0)\n",
        "        running_corrects_val += torch.sum(preds == labels.data)\n",
        "      \n",
        "   \n",
        "     # Calculate average losses\n",
        "      epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "      val_loss = val_loss / len(val_dataloader.dataset)\n",
        "      # Calculate accuracy\n",
        "      epoch_acc = running_corrects_train.double() / len(train_dataloader.dataset)\n",
        "      valid_acc = running_corrects_val / float(len(val_dataloader.dataset))\n",
        "      '''\n",
        "      #calcolo media per liveloss\n",
        "      running_loss += loss.detach() * inputs.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "      epoch_acc=running_corrects.float()/float(len(train_dataloader) * BATCH_SIZE )\n",
        "      epoch_loss=running_loss/float(len(train_dataloader) *BATCH_SIZE)\n",
        "      logs['log loss'] = epoch_loss.item()\n",
        "      logs['accuracy'] = epoch_acc.item()\n",
        "      liveloss.update(logs)\n",
        "      if(epoch%5 == 0 ):\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "        if DRAW:\n",
        "          liveloss.draw()\n",
        "      '''\n",
        "      if (valid_acc > best_acc):\n",
        "        best_acc = valid_acc\n",
        "        best_net = copy.deepcopy(net.state_dict())\n",
        "      if(epoch%5 == 0 ):\n",
        "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(val_loss, valid_acc))\n",
        "      scheduler.step()\n",
        "  \n",
        "  net.load_state_dict(best_net)\n",
        "  return net\n",
        "\n",
        "#test function\n",
        "def test(net, test_dataloader):\n",
        "  net.to(DEVICE)\n",
        "  net.train(False)\n",
        "\n",
        "  running_corrects = 0\n",
        "  for images, labels, _ in test_dataloader:\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "    outputs = net(images)\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "    # Update Corrects\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(len(test_dataloader.dataset))\n",
        "  print('Test Accuracy: {}'.format(accuracy))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyxQgSO9XnVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define images transformation\n",
        "train_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "                                    #transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "                                    ])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "                                    #transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "                                    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhd6B2blXnV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#class splits\n",
        "range_classes = np.arange(100)\n",
        "classes= np.array_split(range_classes, 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bHaMVwEXnV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#net = resnet18()\n",
        "net = resnet32()\n",
        "\n",
        "\n",
        "for i in range(int(100/ClASSES_BATCH)):\n",
        "#cambio il numero di classi di output\n",
        "    net.fc = nn.Linear(64, 10+i*10)\n",
        "\n",
        "    #creating dataset for current iteration\n",
        "    train_dataset = CIFAR100(root='data/', classes=classes[i], train=True, download=True, transform=train_transform)\n",
        "    test_dataset = CIFAR100(root='data/', classes=classes[i],  train=False, download=True, transform=test_transform)\n",
        "\n",
        "    #subsetting train set in train and validation\n",
        "    train_indexes,val_indexes=train_test_split(range(len(train_dataset)),test_size=0.2,random_state=42,stratify=train_dataset.targets)\n",
        "    val_dataset=Subset(train_dataset,val_indexes)\n",
        "    train_dataset=Subset(train_dataset,train_indexes)\n",
        "    \n",
        "    #debug length\n",
        "    print('Len Train : {}'.format(len(train_dataset)))\n",
        "    print('Len Valid : {}'.format(len(val_dataset)))\n",
        "    print('Len Test : {}'.format(len(test_dataset)))\n",
        "\n",
        "    if i != 0:\n",
        "      #creating dataset for test on previous classes\n",
        "      previous_classes = np.array([])\n",
        "      for j in range(i):\n",
        "        previous_classes = np.concatenate((previous_classes, classes[j])).astype(int)\n",
        "      test_prev_dataset = CIFAR100(root='data/', classes=previous_classes,  train=False, download=True, transform=test_transform)\n",
        "\n",
        "      #creating dataset for all classes\n",
        "      #all_classes=np.concatenate((current_classes, classes[i]))\n",
        "      #ho modificato questa riga mettendo al posto di current_classes, previous_classes\n",
        "      all_classes = np.concatenate((previous_classes, classes[i]))\n",
        "      test_all_dataset = CIFAR100(root='data/', classes=all_classes,  train=False, download=True, transform=test_transform)\n",
        "\n",
        "      test_prev_dataloader = DataLoader(test_prev_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
        "      test_all_dataloader = DataLoader(test_all_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "    #creating dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
        "    val_dataloader=DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
        "    \n",
        "\n",
        "\n",
        "    net = train(net, train_dataloader,val_dataloader)\n",
        "    print('Test on new classes')\n",
        "    test(net, test_dataloader)\n",
        "\n",
        "    if i!=0:\n",
        "      print('Test on previous classes')\n",
        "      test(net, test_prev_dataloader)\n",
        "      print('Test on all classes')\n",
        "      test(net, test_all_dataloader)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ-217pknKrw",
        "colab_type": "text"
      },
      "source": [
        "LwF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgBFar6hnqfW",
        "colab_type": "text"
      },
      "source": [
        "To implement LwF I follow the slide given by professor, I take the finetuning and I add the distillation loss.\n",
        "I follow the forum to implement if and I inser it in the train phase as loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5RKQPIonOAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#train function + validation\n",
        "def train(net, train_dataloader,val_dataloader):\n",
        "  best_acc=BEST_ACC\n",
        "  cudnn.benchmark #optimizes benchmark\n",
        "  #criterion = nn.CrossEntropyLoss() # for classification, Cross Entropy\n",
        "  pos_weight = torch.ones([128])\n",
        "  criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "  #criterion = nn.BCELoss()#binary CrossEntropyLoss\n",
        "  parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters \n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "  net.to(DEVICE)\n",
        "\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      logs={}\n",
        "      if(epoch%5 == 0 ):\n",
        "        print('-' * 30)\n",
        "        print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n",
        "        for param_group in optimizer.param_groups:\n",
        "          print('Learning rate:{}'.format(param_group['lr']))\n",
        "      \n",
        "      running_loss = 0.0\n",
        "      val_loss = 0.0\n",
        "      running_corrects_train = 0\n",
        "      running_corrects_val = 0\n",
        "      #train\n",
        "      for inputs, labels, index in train_dataloader:\n",
        "          inputs = inputs.to(DEVICE)\n",
        "          labels = labels.to(DEVICE)\n",
        "\n",
        "          net.train(True)\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          # forward\n",
        "          outputs = net(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward() #backward pass: compute gradients\n",
        "          optimizer.step() #update weights based on accumulated gradients\n",
        "\n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects_train += torch.sum(preds == labels.data)\n",
        "\n",
        "\n",
        "      #validation\n",
        "      \n",
        "      for inputs,labels,index in val_dataloader:\n",
        "        inputs,labels=inputs.to(DEVICE),labels.to(DEVICE)\n",
        "        net.train(False)\n",
        "        outputs = net(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        #new part!\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss+=loss.item() * inputs.size(0)\n",
        "        running_corrects_val += torch.sum(preds == labels.data)\n",
        "      \n",
        "   \n",
        "     # Calculate average losses\n",
        "      epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "      val_loss = val_loss / len(val_dataloader.dataset)\n",
        "      # Calculate accuracy\n",
        "      epoch_acc = running_corrects_train.double() / len(train_dataloader.dataset)\n",
        "      valid_acc = running_corrects_val / float(len(val_dataloader.dataset))\n",
        "      '''\n",
        "      #calcolo media per liveloss\n",
        "      running_loss += loss.detach() * inputs.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "      epoch_acc=running_corrects.float()/float(len(train_dataloader) * BATCH_SIZE )\n",
        "      epoch_loss=running_loss/float(len(train_dataloader) *BATCH_SIZE)\n",
        "      logs['log loss'] = epoch_loss.item()\n",
        "      logs['accuracy'] = epoch_acc.item()\n",
        "      liveloss.update(logs)\n",
        "      if(epoch%5 == 0 ):\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "        if DRAW:\n",
        "          liveloss.draw()\n",
        "      '''\n",
        "      if (valid_acc > best_acc):\n",
        "        best_acc = valid_acc\n",
        "        best_net = copy.deepcopy(net.state_dict())\n",
        "      if(epoch%5 == 0 ):\n",
        "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(val_loss, valid_acc))\n",
        "      scheduler.step()\n",
        "  \n",
        "  net.load_state_dict(best_net)\n",
        "  return net\n",
        "\n",
        "#test function\n",
        "def test(net, test_dataloader):\n",
        "  net.to(DEVICE)\n",
        "  net.train(False)\n",
        "\n",
        "  running_corrects = 0\n",
        "  for images, labels, _ in test_dataloader:\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "    outputs = net(images)\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "    # Update Corrects\n",
        "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(len(test_dataloader.dataset))\n",
        "  print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5hMJsLBnaGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "3929799c-2968-4a8d-99a8-92613f6889b9"
      },
      "source": [
        "#net = resnet18()\n",
        "net = resnet32()\n",
        "\n",
        "\n",
        "for i in range(int(100/ClASSES_BATCH)):\n",
        "#cambio il numero di classi di output\n",
        "    net.fc = nn.Linear(64, 10+i*10)\n",
        "\n",
        "    #creating dataset for current iteration\n",
        "    train_dataset = CIFAR100(root='data/', classes=classes[i], train=True, download=True, transform=train_transform)\n",
        "    test_dataset = CIFAR100(root='data/', classes=classes[i],  train=False, download=True, transform=test_transform)\n",
        "\n",
        "    #subsetting train set in train and validation\n",
        "    train_indexes,val_indexes=train_test_split(range(len(train_dataset)),test_size=0.2,random_state=42,stratify=train_dataset.targets)\n",
        "    val_dataset=Subset(train_dataset,val_indexes)\n",
        "    train_dataset=Subset(train_dataset,train_indexes)\n",
        "    \n",
        "    #debug length\n",
        "    print('Len Train : {}'.format(len(train_dataset)))\n",
        "    print('Len Valid : {}'.format(len(val_dataset)))\n",
        "    print('Len Test : {}'.format(len(test_dataset)))\n",
        "\n",
        "    if i != 0:\n",
        "      #creating dataset for test on previous classes\n",
        "      previous_classes = np.array([])\n",
        "      for j in range(i):\n",
        "        previous_classes = np.concatenate((previous_classes, classes[j])).astype(int)\n",
        "      test_prev_dataset = CIFAR100(root='data/', classes=previous_classes,  train=False, download=True, transform=test_transform)\n",
        "\n",
        "      #creating dataset for all classes\n",
        "      #all_classes=np.concatenate((current_classes, classes[i]))\n",
        "      #ho modificato questa riga mettendo al posto di current_classes, previous_classes\n",
        "      all_classes = np.concatenate((previous_classes, classes[i]))\n",
        "      test_all_dataset = CIFAR100(root='data/', classes=all_classes,  train=False, download=True, transform=test_transform)\n",
        "\n",
        "      test_prev_dataloader = DataLoader(test_prev_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
        "      test_all_dataloader = DataLoader(test_all_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "    #creating dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
        "    val_dataloader=DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
        "    \n",
        "\n",
        "\n",
        "    net = train(net, train_dataloader,val_dataloader)\n",
        "    print('Test on new classes')\n",
        "    test(net, test_dataloader)\n",
        "\n",
        "    if i!=0:\n",
        "      print('Test on previous classes')\n",
        "      test(net, test_prev_dataloader)\n",
        "      print('Test on all classes')\n",
        "      test(net, test_all_dataloader)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Len Train : 4000\n",
            "Len Valid : 1000\n",
            "Len Test : 1000\n",
            "------------------------------\n",
            "Epoch 1/70\n",
            "Learning rate:0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f10f0e5f567c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test on new classes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-cf0f38fb9949>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#backward pass: compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#update weights based on accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    615\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([128])) must be the same as input size (torch.Size([128, 10]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKcWlQ1inacE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}